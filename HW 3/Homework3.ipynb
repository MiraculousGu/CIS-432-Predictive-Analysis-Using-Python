{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-beb82d1cf0e76f0e",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "<h1>Homework 3: Python programming</h1>\n",
                "<h2>Predictive Analytics using Python (CIS432)</h2>\n",
                "<h3>Simon Business School</h3>\n",
                "\n",
                "__Instructor__: Yaron Shaposhnik\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-bf47835b7a801cc7",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## Instructions\n",
                "\n",
                "In this homework assignment, we will practice Python programming. We will keep working with the HELOC dataset, this time performing various analysis tasks that could not be directly performed using pandas. \n",
                "\n",
                "As part of this assignment, you will explore Python's\n",
                "* Data types\n",
                "* Data structures\n",
                "* Flow control \n",
                "* Functions\n",
                "\n",
                "As well as import packages and work with data files."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-c9473db3b6b0d741",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## Answer key\n",
                "\n",
                "This assignment (as well as others) is graded by comparing your answers (that is, the variables and Python objects you create) with precomputed answers. This allows you to get immediate feedback in order to find your errors and correct them. The downside of this approach is that the grading code is strict and even slight deviations from the desired outputs could result in reduction of points. \n",
                "\n",
                "To make this learning experience more efficient, the objects that you are asked to generate are provided to you in the variable `ANSWER_KEY`. Questions may ask you to assign some value (like a number or object such as data frame) to some variable. \n",
                "For example, you might be asked to assign the variable `n_rows` with some value. To view the correct answer simply run the command `ANSWER_KEY['n_rows']`. \n",
                "\n",
                "Note that the answer key is provided to you __for debugging purposes only__. Using it in your final submission or hard-coding solutions __will be considered plagiarism__ and be reported to the student disciplinary committee."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "imports",
                    "locked": true,
                    "points": 10,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# you may ignore this cell\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pickle\n",
                "\n",
                "if os.name == 'nt':\n",
                "    ANSWER_KEY_FILE_NAME = \"answer_key(win).p\"\n",
                "elif os.name == 'posix':\n",
                "    ANSWER_KEY_FILE_NAME = \"answer_key(unix).p\"\n",
                "else:\n",
                "    raise Exception('The code was not tested on',os.name)\n",
                "\n",
                "GENERATE_ANSWER_KEY=False\n",
                "\n",
                "if GENERATE_ANSWER_KEY: \n",
                "    ANSWER_KEY = {} \n",
                "else:        \n",
                "    with open(ANSWER_KEY_FILE_NAME, \"rb\") as f:\n",
                "        ANSWER_KEY = pickle.load( f )             "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-8219ebfbd84c42d7",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "this is the expected value of the variable \"n_rows_df_models\" (which you will be asked to compute) 56\n"
                    ]
                }
            ],
            "source": [
                "# example for using answer key\n",
                "if GENERATE_ANSWER_KEY==False: \n",
                "    print('this is the expected value of the variable \"n_rows_df_models\" (which you will be asked to compute)', ANSWER_KEY['n_rows_df_models'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-8654293b8a6657d3",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "# Beginning of homework 3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-db9d155c39a3bc4b",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "### (q1)\n",
                "The folder `preprocess_datasets` contains multiple variants of the same dataset, which differ by their column values (don't worry for now about how these values were created, this will be the focus of the next homework assignment).\n",
                "\n",
                "Create a list called `preprocessed_datasets` that holds the names of the files in this folder. (Hint: use the OS shell command `ls` to list files)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-7844800c37f1509d",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [],
            "source": [
                "preprocessed_datasets = \"replace this string with your answer\"\n",
                "###\n",
                "preprocessed_datasets = os.listdir(\"./preprocessed_datasets\")\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q1a",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "assert(sorted(preprocessed_datasets) == sorted(ANSWER_KEY['preprocessed_datasets']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-b03ea0037f49a2b0",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Let's look at the file names"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-0914a5a8354dd509",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "heloc_dataset_v1(exc empty rows.).csv_pre(16,quantile).csv\nheloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv\nheloc_dataset_v1(exc empty rows.).csv_pre(-1,manual_true).csv\nheloc_dataset_v1(exc empty rows.).csv_pre(-1,w_binary4missing).csv\nheloc_dataset_v1(exc empty rows.).csv_pre(2,quantile).csv\nheloc_dataset_v1(exc empty rows.).csv_pre(4,quantile).csv\nheloc_dataset_v1(exc empty rows.).csv_pre(8,quantile).csv\n"
                    ]
                }
            ],
            "source": [
                "for f in ANSWER_KEY['preprocessed_datasets']: # notice the syntax, how we iterate over a list of objects (strings), each time f takes a value from that list\n",
                "    print(f)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-6dd3763ec38584a8",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "We observe that the file names follow a pattern: \n",
                "\n",
                "    heloc_dataset_v1(exc empty rows.).csv_pre[PREPROCESSING_METHOD].csv\n",
                "    \n",
                "where [PREPROCESSING_METHOD] could take values such as \"(-1,asis)\" or \"(-1,manual_true)\". Again, for now, don't worry about the meaning of these methods, only that each specifies a certain \"preprocessing\" method. The focus of this assignment is practicing Python programming.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-5e24c68ad7282df5",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Implement the function `file_name_to_preprocessing_method(s)` which takes a filename and extract the preprocessing method. For example, executing the command\n",
                "\n",
                "    file_name_to_preprocessing_method(\"heloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv\")\n",
                "\n",
                "should return the string \"(-1,asis)\"\n",
                "\n",
                "Hint: you may find the string function `.find()` useful."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-274fe4d8a533e688",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [],
            "source": [
                "def file_name_to_preprocessing_method(s): \n",
                "    # execute certain string functions on s, assign the answer to the variable res, and finally return res\n",
                "    # for a list of string functions, see, https://docs.python.org/3/library/string.html \n",
                "    res = \"replace this string with your answer\"\n",
                "    position = s.find('csv_pre')\n",
                "    if position >= 0:\n",
                "        res = s[position+7:-4]\n",
                "    else:\n",
                "        res = 'not found'\n",
                "    return(res)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q1b",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TESTS\n",
                "assert(file_name_to_preprocessing_method('heloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv')=='(-1,asis)'), 'testing file_name_to_preprocessing_method'\n",
                "assert(file_name_to_preprocessing_method('heloc_dataset_v1(exc empty rows.).csv_pre(-1,manual_true).csv')=='(-1,manual_true)'), 'testing file_name_to_preprocessing_method'\n",
                "assert(file_name_to_preprocessing_method('heloc_dataset_v1(exc empty rows.).csv_pre(-1,w_binary4missing).csv')=='(-1,w_binary4missing)'), 'testing file_name_to_preprocessing_method'\n",
                "assert(file_name_to_preprocessing_method('heloc_dataset_v1(exc empty rows.).csv_pre(16,quantile).csv')=='(16,quantile)'), 'testing file_name_to_preprocessing_method'\n",
                "assert(file_name_to_preprocessing_method('heloc_dataset_v1(exc empty rows.).csv_pre(2,quantile).csv')=='(2,quantile)'), 'testing file_name_to_preprocessing_method'\n",
                "assert(file_name_to_preprocessing_method('heloc_dataset_v1(exc empty rows.).csv_pre(4,quantile).csv')=='(4,quantile)'), 'testing file_name_to_preprocessing_method'\n",
                "assert(file_name_to_preprocessing_method('heloc_dataset_v1(exc empty rows.).csv_pre(8,quantile).csv')=='(8,quantile)'), 'testing file_name_to_preprocessing_method'\n",
                "### END TESTS"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-f450741f20dd8ddb",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Implement the function `preprocessing_method_to_file_name(s)` which takes a preprocessing method `s` and returns the respective file name. For example, executing the command:\n",
                "\n",
                "    preprocessing_method_to_file_name(\"(-1,asis)\")\n",
                "\n",
                "should return the string:\n",
                "\n",
                "    \"heloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv\"\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-5ea4517d3f5a4c72",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "3 , 1.2 , True , hello\n"
                    ]
                }
            ],
            "source": [
                "# hint: the following command injects values into the string template defined below\n",
                "i = 3       # an integer\n",
                "f = 1.234   # a floating point number\n",
                "b = True    # a boolean value\n",
                "s = 'hello' # a string\n",
                "x = '%d , %.1f , %r , %s'%(i,f,b,s) # the template injects an integer (%d), floating point number (%.1f - only 1 digit after the decimal point is shown), a boolean (%r this is actually more generic and would also work with other data types), and a string (%s)\n",
                "print(x)    # print the newly created string based on the various data types"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-8b3ab6b48373d403",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'heloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 65
                }
            ],
            "source": [
                "def preprocessing_method_to_file_name(s): \n",
                "    # execute certain string functions on s, assign the answer to the variable res, and finally return res\n",
                "    # for a list of string functions, see, https://docs.python.org/3/library/string.html \n",
                "    res = \"replace this string with your answer\"\n",
                "    for item in preprocessed_datasets:\n",
                "         if s in item:\n",
                "             res = item\n",
                "             break\n",
                "    return(res)\n",
                "\n",
                "preprocessing_method_to_file_name('(-1,asis)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q1c",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TESTS\n",
                "assert(preprocessing_method_to_file_name('(-1,asis)')=='heloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv'), 'test preprocessing_method_to_file_name'\n",
                "assert(preprocessing_method_to_file_name('(-1,manual_true)')=='heloc_dataset_v1(exc empty rows.).csv_pre(-1,manual_true).csv'), 'test preprocessing_method_to_file_name'\n",
                "assert(preprocessing_method_to_file_name('(-1,w_binary4missing)')=='heloc_dataset_v1(exc empty rows.).csv_pre(-1,w_binary4missing).csv'), 'test preprocessing_method_to_file_name'\n",
                "assert(preprocessing_method_to_file_name('(16,quantile)')=='heloc_dataset_v1(exc empty rows.).csv_pre(16,quantile).csv'), 'test preprocessing_method_to_file_name'\n",
                "assert(preprocessing_method_to_file_name('(2,quantile)')=='heloc_dataset_v1(exc empty rows.).csv_pre(2,quantile).csv'), 'test preprocessing_method_to_file_name'\n",
                "assert(preprocessing_method_to_file_name('(4,quantile)')=='heloc_dataset_v1(exc empty rows.).csv_pre(4,quantile).csv'), 'test preprocessing_method_to_file_name'\n",
                "assert(preprocessing_method_to_file_name('(8,quantile)')=='heloc_dataset_v1(exc empty rows.).csv_pre(8,quantile).csv'), 'test preprocessing_method_to_file_name'\n",
                "### END TESTS"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-dcf24a9e8e3dd598",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "This example is quite common in practice. We will often work with multiple files that share common structure and which differ by some attributes. For example, we could have multiple files for different days and we will need to automate the code that processes each file and extracts some information from the file name (e.g., the date). "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-5d90c09e32a6a322",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "### (q2)\n",
                "\n",
                "We do a similar exercise with the folder `models`. The folder contains a list of files, each corresponding to a preprocessing method (e.g., \"(-1,asis)\"), and a model (e.g., \"(Log. Reg.)\"). Again, don't worry about the what the model does or even mean. The key thing to understand here is that each file in `models` relates to a preprocessing method and a model, both of which are specified in the file name: \n",
                "\n",
                "    \"heloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv_model(Log. Reg.).csv\"\n",
                "    \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-51779cc2c11b083e",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Create a list called `models` that holds the names of the files in the folder `models`. (Hint: use the OS shell command `ls` to list files)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-7844800c37f150x",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [],
            "source": [
                "models = \"replace this string with your answer\"\n",
                "###\n",
                "models = os.listdir('./models')\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q2a",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "assert(sorted(models) == sorted(ANSWER_KEY['models']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-e313a79385a1f014",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Implement the function `file_name_to_preprocessing_method_and_model(s)` that takes a model file name and returns two values: the preprocessing method and the model. For example, running the command: \n",
                "\n",
                "    file_name_to_preprocessing_method_and_model(\"heloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv_model(Log. Reg.).csv\")\n",
                "    \n",
                "returns the tuple:\n",
                "\n",
                "    ('(-1,asis)', '(Log. Reg.)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-8f6ebeb77325607a",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "('(-1,asis)', '(Log. Reg.)')"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 69
                }
            ],
            "source": [
                "def file_name_to_preprocessing_method_and_model(s): \n",
                "    # use string functions on s, assign the answer to the variables preprocessing and model, and finally return the tuple: (preprocessing, model)\n",
                "    # for a list of string functions, see, https://docs.python.org/3/library/string.html \n",
                "    preprocessing = \"replace this string with your answer\"\n",
                "    model = \"replace this string with your answer\"\n",
                "    trash,pre,mo = s.split(\"csv_\")\n",
                "    \n",
                "    #find preprocessing\n",
                "    position = pre.find('pre')\n",
                "    if position >= 0:\n",
                "        preprocessing = pre[position+3:-1]\n",
                "    else:\n",
                "        preprocessing = 'not found'\n",
                "    \n",
                "    #find model\n",
                "    position = mo.find('model')\n",
                "    if position >= 0:\n",
                "        model = mo[position+5:-4]\n",
                "    else:\n",
                "        model = 'not found'\n",
                "    return(preprocessing, model)\n",
                "\n",
                "file_name_to_preprocessing_method_and_model('heloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv_model(Log. Reg.).csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q2b",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-fed34a3269595ef3",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Now implement the opposite function `preprocessing_method_and_model_to_file_name(p,m)` that takes a preprocessing method `p` and a model `m` and returns the corresponding file name. For example, running the command: \n",
                "\n",
                "    preprocessing_method_and_model_to_file_name('(8,quantile)','(SVM (RBF))')\n",
                "    \n",
                "returns the string: \n",
                "\n",
                "    'heloc_dataset_v1(exc empty rows.).csv_pre(8,quantile).csv_model(SVM (RBF)).csv'\n",
                "    \n",
                "Note that both `p` and `m` are strings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-7d4ac39aa5d26049",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "'heloc_dataset_v1(exc empty rows.).csv_pre(8,quantile).csv_model(SVM (RBF)).csv'"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 71
                }
            ],
            "source": [
                "def preprocessing_method_and_model_to_file_name(p,m): \n",
                "    # execute certain string functions on s, assign the answer to the variable res, and finally return res\n",
                "    # for a list of string functions, see, https://docs.python.org/3/library/string.html \n",
                "    res = \"replace this string with your answer\"\n",
                "    for item in models:\n",
                "         if p in item and m in item:\n",
                "             res = item\n",
                "             break\n",
                "    return(res)\n",
                "\n",
                "preprocessing_method_and_model_to_file_name('(8,quantile)','(SVM (RBF))')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q2c",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-11cb981c0c1ba01a",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "### (q3)\n",
                "\n",
                "Let's perform a quick sanity check on our data and check the dimensions of each data set. \n",
                "\n",
                "Create the dataframe `df_models` which contains a row per each file in the folder `models` and three columns labeled 'preprocessing', 'model', and 'file', which respectively hold the preprocessing method, the model, and the relative location of the file. \n",
                "\n",
                "The first two columns can be computed using the function `file_name_to_preprocessing_method_and_model(f)` you implemented earlier. To obtain the relative location of the file, you may use the function `join` as demonstrated below. \n",
                "\n",
                "Hint: to create a dataframe you may first create a list of dictionaries, each corresponding to a row in the data frame (and a file in the folder models). In each dictionary the key would be the column names and the dictionary value would be the respective value. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-8650460bfba710f5",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "models/heloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv_model(AdaBoost).csv\n"
                    ]
                }
            ],
            "source": [
                "# sample code for join\n",
                "from os.path import join\n",
                "model_file = 'heloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv_model(AdaBoost).csv'\n",
                "print(join('models',model_file)) # we simply concatenated the folder to the file name so that we could easily access the file when needed"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-f34f6a62e607da1a",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "The function `join` concatenates the folder location (this is a string) with the file name (another string). While this can be done using the '+' operator, in different operating systems different characters are used to describe paths (e.g., it could be '/' or '\\\\'). The function `join` takes care of this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 91,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-8fc9a6814bd56e12",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "           preprocessing        model  \\\n",
                            "0              (-1,asis)  (SVM (RBF))   \n",
                            "1           (4,quantile)        (KNN)   \n",
                            "2  (-1,w_binary4missing)       (CART)   \n",
                            "3          (16,quantile)  (SVM (RBF))   \n",
                            "4          (16,quantile)        (KNN)   \n",
                            "\n",
                            "                                                file  \n",
                            "0  models/heloc_dataset_v1(exc empty rows.).csv_p...  \n",
                            "1  models/heloc_dataset_v1(exc empty rows.).csv_p...  \n",
                            "2  models/heloc_dataset_v1(exc empty rows.).csv_p...  \n",
                            "3  models/heloc_dataset_v1(exc empty rows.).csv_p...  \n",
                            "4  models/heloc_dataset_v1(exc empty rows.).csv_p...  "
                        ],
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessing</th>\n      <th>model</th>\n      <th>file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(-1,asis)</td>\n      <td>(SVM (RBF))</td>\n      <td>models/heloc_dataset_v1(exc empty rows.).csv_p...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(4,quantile)</td>\n      <td>(KNN)</td>\n      <td>models/heloc_dataset_v1(exc empty rows.).csv_p...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(-1,w_binary4missing)</td>\n      <td>(CART)</td>\n      <td>models/heloc_dataset_v1(exc empty rows.).csv_p...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(16,quantile)</td>\n      <td>(SVM (RBF))</td>\n      <td>models/heloc_dataset_v1(exc empty rows.).csv_p...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(16,quantile)</td>\n      <td>(KNN)</td>\n      <td>models/heloc_dataset_v1(exc empty rows.).csv_p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {},
                    "execution_count": 91
                }
            ],
            "source": [
                "df_models = \"replace this string with your answer\"\n",
                "\n",
                "#files\n",
                "model_files = os.listdir('models/')\n",
                "files = [join('models',model) for model in model_files]\n",
                "\n",
                "#preprocessing and models\n",
                "preprocessing = []\n",
                "models = []\n",
                "for f in files:\n",
                "    preprocessing.append(file_name_to_preprocessing_method_and_model(f)[0])\n",
                "    models.append(file_name_to_preprocessing_method_and_model(f)[1])\n",
                "\n",
                "df_models = {\n",
                "    'preprocessing' : preprocessing,\n",
                "    'model' : models,\n",
                "    'file' : files\n",
                "}\n",
                "\n",
                "df_models = pd.DataFrame(df_models)\n",
                "df_models.head()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 92,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-271631831212dbe0",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "   preprocessing        model  \\\n",
                            "0  (16,quantile)   (AdaBoost)   \n",
                            "1  (16,quantile)       (CART)   \n",
                            "2  (16,quantile)        (KNN)   \n",
                            "3  (16,quantile)  (Log. Reg.)   \n",
                            "4  (16,quantile)         (RF)   \n",
                            "\n",
                            "                                                file  \n",
                            "0  models/heloc_dataset_v1(exc empty rows.).csv_p...  \n",
                            "1  models/heloc_dataset_v1(exc empty rows.).csv_p...  \n",
                            "2  models/heloc_dataset_v1(exc empty rows.).csv_p...  \n",
                            "3  models/heloc_dataset_v1(exc empty rows.).csv_p...  \n",
                            "4  models/heloc_dataset_v1(exc empty rows.).csv_p...  "
                        ],
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessing</th>\n      <th>model</th>\n      <th>file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(16,quantile)</td>\n      <td>(AdaBoost)</td>\n      <td>models/heloc_dataset_v1(exc empty rows.).csv_p...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(16,quantile)</td>\n      <td>(CART)</td>\n      <td>models/heloc_dataset_v1(exc empty rows.).csv_p...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(16,quantile)</td>\n      <td>(KNN)</td>\n      <td>models/heloc_dataset_v1(exc empty rows.).csv_p...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(16,quantile)</td>\n      <td>(Log. Reg.)</td>\n      <td>models/heloc_dataset_v1(exc empty rows.).csv_p...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(16,quantile)</td>\n      <td>(RF)</td>\n      <td>models/heloc_dataset_v1(exc empty rows.).csv_p...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {},
                    "execution_count": 92
                }
            ],
            "source": [
                "# Sample of the output\n",
                "ANSWER_KEY['df_models'].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 93,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q3a",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TEST (DO NOT REMOVE CELL)\n",
                "sol = ANSWER_KEY['df_models'].sort_values(by='file',ignore_index=True)\n",
                "diff = sol.compare(df_models.sort_values(by='file',ignore_index=True), keep_equal=False, align_axis=0)\n",
                "assert(len(diff)==0), 'testing df_models'\n",
                "### END TEST "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-54b422a699b800a0",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Create the following variables and assign to them the indicated values: \n",
                "- `n_rows_df_models`: the number of rows in df_models\n",
                "- `n_cols_df_models`: the number of columns in df_models\n",
                "- `num_preprocessing`: the number of different preprocessing methods\n",
                "- `num_models`: the number of different models\n",
                "- `models_list`: the list of models that appear in the files in the folder `models`; sort the list using the command `sorted`\n",
                "\n",
                "Hint: you may use the series function `.unique()` to return the unique values of a certain column. (see also, https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-018ec02e20d610cf",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# sample code\n",
                "sorted([1,10,3])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 109,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-ae76d92ac64d886d",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [],
            "source": [
                "n_rows_df_models = \"replace this string with your answer\"\n",
                "n_cols_df_models = \"replace this string with your answer\"\n",
                "num_preprocessing = \"replace this string with your answer\"\n",
                "num_models = \"replace this string with your answer\"\n",
                "models_list = \"replace this string with your answer\"\n",
                "\n",
                "# number of rows\n",
                "n_rows_df_models = len(df_models)\n",
                "\n",
                "# number of cols\n",
                "n_cols_df_models = len(df_models.columns)\n",
                "\n",
                "#number of different preprocessing methods\n",
                "num_preprocessing = len(pd.unique(df_models['preprocessing']))\n",
                "\n",
                "#number of models\n",
                "num_models = len(pd.unique(df_models['model']))\n",
                "\n",
                "#model lists\n",
                "models_list = sorted(pd.unique(df_models['model']))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 110,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q3b",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TEST (DO NOT REMOVE CELL)\n",
                "assert(ANSWER_KEY['n_rows_df_models']==n_rows_df_models)\n",
                "assert(ANSWER_KEY['n_cols_df_models']==n_cols_df_models)\n",
                "assert(ANSWER_KEY['num_preprocessing']==num_preprocessing)\n",
                "assert(ANSWER_KEY['num_models']==num_models)\n",
                "assert(sorted(ANSWER_KEY['models_list'])==sorted(models_list))\n",
                "### END TEST "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-e519514103b78914",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Let's examine the resulting dataframe:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 111,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-8a0b8566933b729c",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "The number of rows in df_models: 56\nThe number of columns in df_models: 3\nThe number of different preprocessing methods: 7\nThe number of different models: 8\nThe list of models: ['(AdaBoost)', '(CART)', '(KNN)', '(Log. Reg.)', '(RF)', '(SVM (Linear))', '(SVM (Poly))', '(SVM (RBF))']\n"
                    ]
                }
            ],
            "source": [
                "print('The number of rows in df_models:', ANSWER_KEY['n_rows_df_models'])\n",
                "print('The number of columns in df_models:', ANSWER_KEY['n_cols_df_models'])\n",
                "print('The number of different preprocessing methods:', ANSWER_KEY['num_preprocessing'])\n",
                "print('The number of different models:', ANSWER_KEY['num_models'])\n",
                "print('The list of models:', ANSWER_KEY['models_list'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-17c6a1cfe3d70c28",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Let's add a new column to `df_models` which would hold the dimensions (shape) of the data frame in each file. Call the new column `shape`. Each element in this column should be a tuple with two integers indicating the number of rows and columns in the corresponding file.\n",
                "\n",
                "Hint: you may use the command `.apply()` to apply a function to each row (or column) of a data frame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-7dc3eba1b333c931",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "scrolled": true,
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# Sample code (1): for simplicity, we will apply a function to a smaller dataframe that only contains the first 3 rows of df_models \n",
                "df_models.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-786649c79291f7ab",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# Sample code (2)\n",
                "\n",
                "def some_function_to_apply_to_each_row(row):\n",
                "    # The function we define here counts the number of characters in the column 'model' (not a particularly interesting function, just an example)\n",
                "    return(len(row['model']))\n",
                "\n",
                "# The function apply returns the same number of elements as the number of rows in the dataframe to which it was applied\n",
                "df_models.head(3).apply(some_function_to_apply_to_each_row, axis=1) # the axis parameter means that we apply the function to each row (rather than to each column)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-7a63f1d4cc6934d5",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# Sample code (3): an alternative way of defining the same function using \"anonymous functions\"\n",
                "some_function_to_apply_to_each_row = lambda row:len(row['model']) # here we define the same function more compactly\n",
                "\n",
                "df_models.head(3).apply(some_function_to_apply_to_each_row, axis=1) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-eecd57cdbae15fae",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# Sample code (4): an even more compact way to do it\n",
                "df_models.head(3).apply(lambda row:len(row['model']), axis=1) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-f343aaa8f8011a50",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [],
            "source": [
                "# write here code that adds the new column to df_models (running it may take a while as you might read each of the files in the folder models)\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-de43cca3c7f300df",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# example of the output\n",
                "ANSWER_KEY['df_models_update1'].head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q3c",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TEST (DO NOT REMOVE CELL)\n",
                "sol = ANSWER_KEY['df_models_update1'].sort_values(by='file',ignore_index=True)\n",
                "diff = sol.compare(df_models.sort_values(by='file',ignore_index=True), keep_equal=False, align_axis=0)\n",
                "assert(len(diff)==0), 'testing df_models_update1'\n",
                "### END TEST "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-54d4f46acf4177a7",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Use the function `.apply()` to add two new columns called `n_rows` and `n_cols`, respectively holding the number of rows and columns in each row. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-bcb1fe40a4027f25",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [],
            "source": [
                "# write here code that adds the new column to df_models\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-2688475db658ff77",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# example of the output\n",
                "ANSWER_KEY['df_models_update2'].head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q3d",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TEST (DO NOT REMOVE CELL)\n",
                "sol = ANSWER_KEY['df_models_update2'].sort_values(by='file',ignore_index=True)\n",
                "diff = sol.compare(df_models.sort_values(by='file',ignore_index=True), keep_equal=False, align_axis=0)\n",
                "assert(len(diff)==0), 'testing df_models_update2'\n",
                "### END TEST "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-8268018e2c36bbe9",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Let's look at the number of rows and columns in each file:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-0d9a2c1472e5cb2c",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "ANSWER_KEY['df_models_update2'].describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-6c95eac27afae2b6",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "The number of rows is always 9871 and the number of columns varies between 24 to 381 (this depends on the preprocessing method)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-dfa4eb5b43d02657",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "### (q4) \n",
                "\n",
                "The files in the folder models contain the output of predictive models trained on the same dataset that was preprocessed in a certain ways. For example, the file \n",
                "\n",
                "    heloc_dataset_v1(exc empty rows.).csv_pre(-1,asis).csv_model(AdaBoost).csv\n",
                "    \n",
                "contains the dataset that was preprocessed using the method `(-1,asis)` (whatever this means) and to which the model `(AdaBoost)` (whatever this means) was applied. The column `RiskPerformance` in that datafile contains the prediction of this model which was trained on the particular preprocessed data. \n",
                "\n",
                "In what follows, we compare the predictions made by each of these models and preprocessing methods. \n",
                "\n",
                "Key takeaway: each files in the folder model contains one columns that is of interest to us called `RiskPerformance`. This column always holds 9871 values and in this question we will compare the columns corresponding to the different files."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-a5ca268449aa1862",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Create the dataframe `df_predictions` which holds the columns of `df_models` and additional 9871 columns, one per each prediction made by the respective model and preprocessing method for the specific observation. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-9860183d7362245d",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [],
            "source": [
                "df_predictions = \"replace this string with your answer\"\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-b074acd44e5f1ace",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "scrolled": true,
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# Example of the first 5 rows of the output\n",
                "ANSWER_KEY['df_predictions'].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-6e32ff7188f470b4",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "(we basically added 9871 columns to each row using values from the respective file) "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-37c9506785054907",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "To emphasize that the data in `df_predictions` comes from the files specified in the column `file`, below we print the prediction from the two different sources."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-2ec1315b1253b977",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# Here we print predictions from the file \n",
                "print('File name:', df_models.iloc[0].loc['file'])\n",
                "print('Prediction for the first 6 customers')\n",
                "pd.read_csv(df_models.iloc[0].loc['file'])['RiskPerformance'].head(6)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-b659e933b6e37686",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# Here we print the first row of df_predictions (only first 12 columns out of ~9800)\n",
                "ANSWER_KEY['df_predictions'].iloc[[0],:12]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q4a",
                    "locked": true,
                    "points": 10,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TEST (DO NOT REMOVE CELL)\n",
                "sol = ANSWER_KEY['df_predictions'].sort_values(by='file',ignore_index=True)\n",
                "diff = sol.compare(df_predictions.sort_values(by='file',ignore_index=True), keep_equal=False, align_axis=0)\n",
                "assert(len(diff)==0), 'testing df_predictions'\n",
                "### END TEST "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-ecf06cff546f762e",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Let's focus on one preprocessing method: `(-1,asis)`. Set the variable `df_predictions_one_method` to hold only the rows in `df_predictions` where the preprocessing method is `(-1,asis)`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-6cd6c2bc9cddc0db",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [],
            "source": [
                "df_predictions_one_method = \"replace this string with your answer\"\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q4b",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TEST (DO NOT REMOVE CELL)\n",
                "sol = ANSWER_KEY['df_predictions_one_method'].sort_values(by='file',ignore_index=True)\n",
                "diff = sol.compare(df_predictions_one_method.sort_values(by='file',ignore_index=True), keep_equal=False, align_axis=0)\n",
                "assert(len(diff)==0), 'testing df_predictions_one_method'\n",
                "### END TEST "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-85452c09460de174",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# output\n",
                "ANSWER_KEY['df_predictions_one_method']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-86786df929f7d16a",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Create the matrix `pairwise_comparison` of size 8x8 where the value of the matrix element (i,j) is the similarity between the predictions in rows i and j. \n",
                "\n",
                "For example, suppose that in row 1 the prediction were [Bad,Bad,Good,Good] and that in row 2 the prediction were [Bad, Bad, Good, Bad]. These predictions coincide in 75% of cases (in all but the 4th prediction). In this case, we would like the matrix element (1,2) to be equal to 0.75 (as well as the value of element (2,1)).\n",
                "\n",
                "Hints: \n",
                "- you may iterate over rows and columns using the for loop\n",
                "- you may compare two vectors easily by converting them to numpy arrays\n",
                "- if you convert the compared values (booleans) to 0's and 1's, the average (np.average) would return the similarity\n",
                "- the matrix is symmetric and you can save on running time using an if-else statement that checks if you're below or above the diagonal (if j>i we can use the similarity of i vs. j instead of recomputing it for j vs. i)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-6ba94a51f5adaef8",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# sample code\n",
                "print('this is what happens when we just compare lists:', ['Dog','Cat','Cat'] == ['Dog','Cat','Dog'])\n",
                "comparison = np.array(['Dog','Cat','Cat']) == np.array(['Dog','Cat','Dog'])\n",
                "print('here we compare numpy arrays:', comparison)\n",
                "print('here we convert the comparison to integers:',comparison.astype(int))\n",
                "print('similarity', np.average(comparison),'\\n')\n",
                "print('create a matrix of 0\\'s\\n',np.zeros((2,2)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "__For consistency with the automated solution, sort the dataframe `df_predictions_one_method` prior to generating the matrix according the file column by running the function:__\n",
                "\n",
                "    df_predictions_one_method = df_predictions_one_method.sort_values(by='file',ignore_index=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-8784d24608225537",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [],
            "source": [
                "pairwise_comparison =  \"replace this string with your answer\"\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-7991239c6ed02852",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# sample output\n",
                "print(ANSWER_KEY['pairwise_comparison'].round(2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q4c",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TEST (DO NOT REMOVE CELL)\n",
                "assert(np.equal(ANSWER_KEY['pairwise_comparison'].round(2), pairwise_comparison.round(2)).all()), 'testing pairwise_comparison'\n",
                "### END TEST "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-5d796a3d47d0ee21",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Create the data frame `df_pairwise_comparison` which would hold the same values as `pairwise_comparison` but where the indexes and column names describe the models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-8b61f446e1dc1b0a",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [],
            "source": [
                "df_pairwise_comparison =  \"replace this string with your answer\"\n",
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-5b3e50d38edb5d65",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# expected output\n",
                "ANSWER_KEY['df_pairwise_comparison'].round(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": [
                "# example of the first row and column indexes \n",
                "print(ANSWER_KEY['df_pairwise_comparison'].index.values[0])\n",
                "print(ANSWER_KEY['df_pairwise_comparison'].columns.values[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [],
            "source": [
                "# observe that the data type of the indexes and column names are strings \n",
                "# note that if in your dataframes these types are different the test will fail\n",
                "print(type(ANSWER_KEY['df_pairwise_comparison'].index.values[0]))\n",
                "print(type(ANSWER_KEY['df_pairwise_comparison'].columns.values[0]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q4d",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TEST (DO NOT REMOVE CELL)\n",
                "sol = ANSWER_KEY['df_pairwise_comparison'].round(2)\n",
                "diff = sol.compare(df_pairwise_comparison.round(2), keep_equal=False, align_axis=0)\n",
                "assert(len(diff)==0), 'testing df_pairwise_comparison'\n",
                "### END TEST "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-960906d737b06ad1",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "We can visualize the dataframe `df_pairwise_comparison` using a heatmap:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-3ecf5134a3984115",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "import seaborn as sns\n",
                "fig, ax = plt.subplots(figsize=(6, 4))\n",
                "ax = sns.heatmap(ANSWER_KEY['df_pairwise_comparison']) # notice how compact the code is due to the indexes and column names of the dataframe\n",
                "fig.suptitle('Pairwise comparison between models\\' predictions');"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-48a5552bcbd4e050",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Comparing the predictions that different models make on the same data, we observe that overall the models operate quite differently. The closest models are Logistic regression (Log. Reg.) and Linear SVM, both of which are linear models (we will discuss these models in detail later in the course).\n",
                "   \n",
                "    \n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-ff79dd76cf3bd045",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "### (q5)\n",
                "Finally, let's export the pairwise comparison to files for future use. First as a csv file and then as a json file. In both case, round the values in the dataframe `df_pairwise_comparison` to two digits after the decimal point using the dataframe function `round()`.\n",
                "\n",
                "Use the dataframe function `to_csv` to export the dataframe `df_pairwise_comparison` as CSV to the file `df_pairwise_comparison.csv`. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-397eb9051b02c326",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                }
            },
            "outputs": [],
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q5a",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TEST (DO NOT REMOVE CELL)\n",
                "sol = df_pairwise_comparison.round(2)\n",
                "diff = sol.compare(pd.read_csv('df_pairwise_comparison.csv', index_col=0).round(2), keep_equal=False, align_axis=0)\n",
                "assert(len(diff)==0), 'testing csv export'\n",
                "### END TEST "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-2e0a0d2aef5fee21",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "The image below shows how the csv file looks like:\n",
                "\n",
                "<img src=\"csv.png\">"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-3a8ce59da5e9d842",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Use the dataframe function `to_json` to export the dataframe `df_pairwise_comparison` as JSON the file `df_pairwise_comparison.json` (use the parameter `orient=\"split\"` and `indent=2`).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-57e4be0724ed947e",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true,
                    "task": false
                },
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-0cac1e41dd9e4468",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "The image below shows how the json file looks like: ('dictionary' with 'columns' holding the list of column names, 'index' holding the list of indexes, and 'data' which holds a matrix represented as a list of lists; the first list holds the values in the first row, the second list holds values in the second row, etc.)\n",
                "\n",
                "<img src=\"json.png\">"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "q5b",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TEST (DO NOT REMOVE CELL)\n",
                "sol = df_pairwise_comparison.round(2)\n",
                "diff = sol.compare(pd.read_json('df_pairwise_comparison.json', orient=\"split\").round(2), keep_equal=False, align_axis=0)\n",
                "assert(len(diff)==0), 'testing csv export'\n",
                "### END TEST "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-a31e421e4a6b064f",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "# Summary\n",
                "\n",
                "\n",
                "In this homework assignment, we practiced Python programming. Working with the HELOC dataset, we automated the loading of data files, extracted information from their names, and compared their content. We explored various data types, data structures, flow control methods and wrote functions. The data files we worked with contains information about preprocessing methods and models. In the next assignment, we will explore these at a greater depth. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {
                "nbgrader": {
                    "grade": true,
                    "grade_id": "cell-56a7f83815146078",
                    "locked": true,
                    "points": 0,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "### BEGIN TEST (DO NOT REMOVE CELL)\n",
                "if GENERATE_ANSWER_KEY: \n",
                "    with open(ANSWER_KEY_FILE_NAME, \"wb\") as f:\n",
                "        pickle.dump( ANSWER_KEY,  f)\n",
                "### END TEST "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-de4459e60753beb5",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false,
                    "task": false
                },
                "editable": false,
                "deletable": false
            },
            "source": []
        }
    ],
    "metadata": {
        "celltoolbar": "Create Assignment",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5-final"
        },
        "latex_envs": {
            "LaTeX_envs_menu_present": true,
            "autoclose": false,
            "autocomplete": true,
            "bibliofile": "biblio.bib",
            "cite_by": "apalike",
            "current_citInitial": 1,
            "eqLabelWithNumbers": true,
            "eqNumInitial": 1,
            "hotkeys": {
                "equation": "Ctrl-E",
                "itemize": "Ctrl-I"
            },
            "labels_anchors": false,
            "latex_user_defs": false,
            "report_style_numbering": false,
            "user_envs_cfg": false
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": [],
            "number_sections": false,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {
                "height": "calc(100% - 180px)",
                "left": "10px",
                "top": "150px",
                "width": "281px"
            },
            "toc_section_display": true,
            "toc_window_display": true
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}